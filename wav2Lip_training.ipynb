{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "rxlWwtUG3ci8",
        "outputId": "2c17c10b-4492-4c40-966a-869a68aed1da"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2c8f482f-e989-494a-a5e6-84b8a01d008b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2c8f482f-e989-494a-a5e6-84b8a01d008b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.16.tar.gz (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.16-py3-none-any.whl size=110685 sha256=4b55b03f1b3e60231d2386a629e73b181d31c0a5f25026aab6053f885707e5d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/4b/fb/736478af5e8004810081a06259f9aa2f7c3329fc5d03c2c412\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.15\n",
            "    Uninstalling kaggle-1.5.15:\n",
            "      Successfully uninstalled kaggle-1.5.15\n",
            "Successfully installed kaggle-1.5.16\n",
            "kaggle.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "#if you've already uploaded kaggle.json before, don't do it again\n",
        "if not(os.path.exists(\"kaggle.json\")):\n",
        "  files.upload() #upload kaggle.json\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzpqM1ze9rk7",
        "outputId": "df2a00a5-7004-4279-85e2-356a14258ff7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.65.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.4)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2023.5.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.27.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.26.16)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.4)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty7Gp3wL9cax",
        "outputId": "e4e4ac28-3561-415f-f801-d38e4932b74f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading girdcorpus-s3.zip to ./girdcorpus-s3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.91G/1.91G [01:47<00:00, 19.1MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "import pandas\n",
        "\n",
        "od.download(\n",
        "    \"https://www.kaggle.com/datasets/ahmedmohamed365/girdcorpus-s3\")\n",
        "od.download(\n",
        "    \"https://www.kaggle.com/datasets/ahmedmohamed365/gridcorpus1\")\n",
        "od.download(\n",
        "    \"https://www.kaggle.com/datasets/ahmedmohamed365/gridcorpus-s4\")\n",
        "od.download(\n",
        "    \"https://www.kaggle.com/datasets/ahmedmohamed365/lombard-gridfront\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYJsUDAj_CO4",
        "outputId": "dbccc41d-34d4-4f4d-e57c-cdc99cef7078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Wav2Lip'...\n",
            "remote: Enumerating objects: 378, done.\u001b[K\n",
            "remote: Total 378 (delta 0), reused 0 (delta 0), pack-reused 378\u001b[K\n",
            "Receiving objects: 100% (378/378), 526.97 KiB | 2.15 MiB/s, done.\n",
            "Resolving deltas: 100% (205/205), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/zabique/Wav2Lip.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC9Y_ra8-mFP",
        "outputId": "258a5c9e-7694-4bfc-dfdf-2722ca63c3cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from -r /content/Wav2Lip/requirements.txt (line 1)) (0.10.0.post2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r /content/Wav2Lip/requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from -r /content/Wav2Lip/requirements.txt (line 3)) (4.7.0.72)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r /content/Wav2Lip/requirements.txt (line 4)) (4.7.0.72)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r /content/Wav2Lip/requirements.txt (line 5)) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r /content/Wav2Lip/requirements.txt (line 6)) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r /content/Wav2Lip/requirements.txt (line 7)) (4.65.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from -r /content/Wav2Lip/requirements.txt (line 8)) (0.56.4)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/Wav2Lip/requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/Wav2Lip/requirements.txt (line 1)) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/Wav2Lip/requirements.txt (line 1)) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/Wav2Lip/requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/Wav2Lip/requirements.txt (line 1)) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/Wav2Lip/requirements.txt (line 1)) (0.12.1)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/Wav2Lip/requirements.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/Wav2Lip/requirements.txt (line 1)) (0.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/Wav2Lip/requirements.txt (line 1)) (4.7.1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/Wav2Lip/requirements.txt (line 1)) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/Wav2Lip/requirements.txt (line 1)) (1.0.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/Wav2Lip/requirements.txt (line 5)) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/Wav2Lip/requirements.txt (line 5)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/Wav2Lip/requirements.txt (line 5)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/Wav2Lip/requirements.txt (line 5)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/Wav2Lip/requirements.txt (line 5)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r /content/Wav2Lip/requirements.txt (line 5)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r /content/Wav2Lip/requirements.txt (line 5)) (16.0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->-r /content/Wav2Lip/requirements.txt (line 6)) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r /content/Wav2Lip/requirements.txt (line 6)) (8.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->-r /content/Wav2Lip/requirements.txt (line 8)) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->-r /content/Wav2Lip/requirements.txt (line 8)) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa->-r /content/Wav2Lip/requirements.txt (line 1)) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa->-r /content/Wav2Lip/requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r /content/Wav2Lip/requirements.txt (line 6)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r /content/Wav2Lip/requirements.txt (line 6)) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r /content/Wav2Lip/requirements.txt (line 6)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r /content/Wav2Lip/requirements.txt (line 6)) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->-r /content/Wav2Lip/requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->-r /content/Wav2Lip/requirements.txt (line 1)) (1.15.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r /content/Wav2Lip/requirements.txt (line 5)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r /content/Wav2Lip/requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->-r /content/Wav2Lip/requirements.txt (line 1)) (2.21)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r /content/Wav2Lip/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Z26Daw1H-_4c"
      },
      "outputs": [],
      "source": [
        "temp1 = os.listdir(\"/content/girdcorpus-s3/content/data/mpg_6000\")\n",
        "temp2 = os.listdir(\"/content/gridcorpus-s4/content/preprocessData/mpg_6000\")\n",
        "temp3 = os.listdir(\"/content/gridcorpus1/content/gridCorpus_s1/mpg_6000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GgpB9KPF_UfE"
      },
      "outputs": [],
      "source": [
        "video_names = []\n",
        "\n",
        "for i in temp1:\n",
        "    video_names.append(f\"/kaggle/input/girdcorpus-s3/content/data/mpg_6000/{i}\")\n",
        "\n",
        "for i in temp2:\n",
        "    video_names.append(f\"/kaggle/input/gridcorpus-s4/content/preprocessData/mpg_6000/{i}\")\n",
        "\n",
        "for i in temp3:\n",
        "    video_names.append(f\"/kaggle/input/gridcorpus1/content/gridCorpus_s1/mpg_6000/{i}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfN9JK1QygAT",
        "outputId": "050ae725-b4f2-42e9-9258-2ce113f582f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-07-20 06:21:33--  https://iiitaphyd-my.sharepoint.com/personal/radrabha_m_research_iiit_ac_in/_layouts/15/download.aspx?share=EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA\n",
            "Resolving iiitaphyd-my.sharepoint.com (iiitaphyd-my.sharepoint.com)... 13.107.136.8, 13.107.138.8, 2620:1ec:8f8::8, ...\n",
            "Connecting to iiitaphyd-my.sharepoint.com (iiitaphyd-my.sharepoint.com)|13.107.136.8|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 435801865 (416M) [application/octet-stream]\n",
            "Saving to: ‘/content/Wav2Lip/checkpoints/wav2lip.pth’\n",
            "\n",
            "/content/Wav2Lip/ch 100%[===================>] 415.61M   152MB/s    in 2.7s    \n",
            "\n",
            "2023-07-20 06:21:36 (152 MB/s) - ‘/content/Wav2Lip/checkpoints/wav2lip.pth’ saved [435801865/435801865]\n",
            "\n",
            "--2023-07-20 06:21:41--  https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\n",
            "Resolving www.adrianbulat.com (www.adrianbulat.com)... 45.136.29.207\n",
            "Connecting to www.adrianbulat.com (www.adrianbulat.com)|45.136.29.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89843225 (86M) [application/octet-stream]\n",
            "Saving to: ‘/content/Wav2Lip/face_detection/detection/sfd/s3fd.pth’\n",
            "\n",
            "/content/Wav2Lip/fa 100%[===================>]  85.68M   432KB/s    in 80s     \n",
            "\n",
            "2023-07-20 06:23:03 (1.07 MB/s) - ‘/content/Wav2Lip/face_detection/detection/sfd/s3fd.pth’ saved [89843225/89843225]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget 'https://iiitaphyd-my.sharepoint.com/personal/radrabha_m_research_iiit_ac_in/_layouts/15/download.aspx?share=EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA' -O '/content/Wav2Lip/checkpoints/wav2lip.pth'\n",
        "a = !pip install https://raw.githubusercontent.com/AwaleSajil/ghc/master/ghc-1.0-py3-none-any.whl\n",
        "\n",
        "# !pip uninstall tensorflow tensorflow-gpu\n",
        "\n",
        "#download pretrained model for face detection\n",
        "!wget \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" -O \"/content/Wav2Lip/face_detection/detection/sfd/s3fd.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xJE8TL5jzLvj"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/Wav2Lip/filelists/train.txt\",\"w\") as file:\n",
        "    for i in video_names[20:]:\n",
        "\n",
        "            file.write(i+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Tl2e5Tv-zOvG"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/Wav2Lip/filelists/val.txt\",\"w\") as file:\n",
        "    for i in video_names[0:10]:\n",
        "\n",
        "            file.write(i+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fAK6i6qdzUUg"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/Wav2Lip/filelists/test.txt\",\"w\") as file:\n",
        "    for i in video_names[10:20]:\n",
        "\n",
        "            file.write(i+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BfRuWB5c0fDO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class Conv2d(nn.Module):\n",
        "    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        ########TODO######################\n",
        "        # 按下面的网络结构要求，补全代码\n",
        "        # self.conv_block: Sequential结构，Conv2d+BatchNorm\n",
        "        # self.act: relu激活函数\n",
        "        self.conv_block = nn.Sequential(\n",
        "                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\n",
        "                            nn.BatchNorm2d(cout)\n",
        "                            )\n",
        "        self.act = nn.ReLU()\n",
        "        self.residual = residual\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv_block(x)\n",
        "        if self.residual:\n",
        "            out += x\n",
        "        return self.act(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xLKuJdp31L15"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class SyncNet_color(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SyncNet_color, self).__init__()\n",
        "\n",
        "        ################TODO###################\n",
        "        #根据上面提供的网络结构图，补全下面卷积网络的参数\n",
        "\n",
        "        self.face_encoder = nn.Sequential(\n",
        "            Conv2d(15, 32, kernel_size=(7, 7), stride=1, padding=3),\n",
        "\n",
        "            Conv2d(32, 64, kernel_size=5, stride=(1, 2), padding=1),\n",
        "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "\n",
        "            Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "\n",
        "            Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "\n",
        "            Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
        "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "\n",
        "            Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
        "            Conv2d(512, 512, kernel_size=3, stride=1, padding=0),\n",
        "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n",
        "\n",
        "        self.audio_encoder = nn.Sequential(\n",
        "            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "\n",
        "            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),\n",
        "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "\n",
        "            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),\n",
        "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "\n",
        "            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),\n",
        "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "\n",
        "            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),\n",
        "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n",
        "\n",
        "    def forward(self, audio_sequences, face_sequences): # audio_sequences := (B, dim, T)\n",
        "\n",
        "        #########################TODO#######################\n",
        "        # 正向传播\n",
        "        face_embedding = self.face_encoder(face_sequences)\n",
        "        audio_embedding = self.audio_encoder(audio_sequences)\n",
        "\n",
        "        audio_embedding = audio_embedding.view(audio_embedding.size(0), -1)\n",
        "        face_embedding = face_embedding.view(face_embedding.size(0), -1)\n",
        "\n",
        "        audio_embedding = F.normalize(audio_embedding, p=2, dim=1)\n",
        "        face_embedding = F.normalize(face_embedding, p=2, dim=1)\n",
        "\n",
        "\n",
        "        return audio_embedding, face_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MklIYGwd7D_U",
        "outputId": "bd1733c3-3fb0-4eac-f763-c6931729cea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting hparams\n",
            "  Downloading hparams-0.3.0-py3-none-any.whl (11 kB)\n",
            "Collecting typeguard (from hparams)\n",
            "  Downloading typeguard-4.0.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from typeguard->hparams) (4.7.1)\n",
            "Installing collected packages: typeguard, hparams\n",
            "Successfully installed hparams-0.3.0 typeguard-4.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install hparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Yhg758jk1R5N"
      },
      "outputs": [],
      "source": [
        "from os.path import dirname, join, basename, isfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "%cd Wav2Lip\n",
        "from models import SyncNet_color as SyncNet\n",
        "import audio\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils import data as data_utils\n",
        "import numpy as np\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "import os, random, cv2, argparse\n",
        "from hparams import hparams, get_image_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64BMDeXQ1-Zp",
        "outputId": "2d0f4b76-ca16-4ee0-d12d-8848f3ddbe59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading lombard-gridfront.zip to ./lombard-gridfront\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 23.8G/23.8G [21:49<00:00, 19.5MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "import pandas\n",
        "\n",
        "od.download(\n",
        "    \"https://www.kaggle.com/datasets/ahmedmohamed365/lombard-gridfront\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNkmg8sR5ltE"
      },
      "outputs": [],
      "source": [
        "global_step = 0 #起始的step\n",
        "global_epoch = 0 #起始的epoch\n",
        "use_cuda = torch.cuda.is_available()#训练的设备 cpu or gpu\n",
        "print('use_cuda: {}'.format(use_cuda))\n",
        "\n",
        "syncnet_T = 5 ## 每次选取200ms的视频片段进行训练，视频的fps为25，所以200ms对应的帧数为：25*0.2=5帧\n",
        "syncnet_mel_step_size = 16 # 200ms对应的声音的mel-spectrogram特征的长度为16.\n",
        "data_root=\"/content/lombard-gridfront/lombard_front_audio\" #数据集的位置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2dEFlyKTPRR"
      },
      "outputs": [],
      "source": [
        "class Dataset(object):\n",
        "    def __init__(self, split):\n",
        "        self.all_videos = get_image_list(data_root, split)\n",
        "\n",
        "    def get_frame_id(self, frame):\n",
        "        return int(basename(frame).split('.')[0])\n",
        "\n",
        "    def get_window(self, start_frame):\n",
        "        start_id = self.get_frame_id(start_frame)\n",
        "        vidname = dirname(start_frame)\n",
        "\n",
        "        window_fnames = []\n",
        "        for frame_id in range(start_id, start_id + syncnet_T):\n",
        "            frame = join(vidname, '{}.jpg'.format(frame_id))\n",
        "            if not isfile(frame):\n",
        "                return None\n",
        "            window_fnames.append(frame)\n",
        "        return window_fnames\n",
        "\n",
        "    def crop_audio_window(self, spec, start_frame):\n",
        "        # num_frames = (T x hop_size * fps) / sample_rate\n",
        "        start_frame_num = self.get_frame_id(start_frame)\n",
        "        start_idx = int(80. * (start_frame_num / float(hparams.fps)))\n",
        "\n",
        "        end_idx = start_idx + syncnet_mel_step_size\n",
        "\n",
        "        return spec[start_idx : end_idx, :]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_videos)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        return: x,mel,y\n",
        "        x: 五张嘴唇图片\n",
        "        mel：对应的语音的mel spectrogram\n",
        "        t：同步or不同步\n",
        "\n",
        "        \"\"\"\n",
        "        while 1:\n",
        "            idx = random.randint(0, len(self.all_videos) - 1)\n",
        "            vidname = self.all_videos[idx]\n",
        "\n",
        "            img_names = list(glob(join(vidname, '*.jpg')))\n",
        "            if len(img_names) <= 3 * syncnet_T:\n",
        "                continue\n",
        "            img_name = random.choice(img_names)\n",
        "            wrong_img_name = random.choice(img_names)\n",
        "            while wrong_img_name == img_name:\n",
        "                wrong_img_name = random.choice(img_names)\n",
        "\n",
        "\n",
        "            #随机决定是产生负样本还是正样本\n",
        "            if random.choice([True, False]):\n",
        "                y = torch.ones(1).float()\n",
        "                chosen = img_name\n",
        "            else:\n",
        "                y = torch.zeros(1).float()\n",
        "                chosen = wrong_img_name\n",
        "\n",
        "            window_fnames = self.get_window(chosen)\n",
        "            if window_fnames is None:\n",
        "                continue\n",
        "\n",
        "            window = []\n",
        "            all_read = True\n",
        "            for fname in window_fnames:\n",
        "                img = cv2.imread(fname)\n",
        "                if img is None:\n",
        "                    all_read = False\n",
        "                    break\n",
        "                try:\n",
        "                    img = cv2.resize(img, (hparams.img_size, hparams.img_size))\n",
        "                except Exception as e:\n",
        "                    all_read = False\n",
        "                    break\n",
        "\n",
        "                window.append(img)\n",
        "\n",
        "            if not all_read: continue\n",
        "\n",
        "            try:\n",
        "                wavpath = join(vidname, \"audio.wav\")\n",
        "                wav = audio.load_wav(wavpath, hparams.sample_rate)\n",
        "\n",
        "                orig_mel = audio.melspectrogram(wav).T\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "            mel = self.crop_audio_window(orig_mel.copy(), img_name)\n",
        "\n",
        "            if (mel.shape[0] != syncnet_mel_step_size):\n",
        "                continue\n",
        "\n",
        "            # H x W x 3 * T\n",
        "            x = np.concatenate(window, axis=2) / 255.\n",
        "            x = x.transpose(2, 0, 1)\n",
        "            x = x[:, x.shape[1]//2:]\n",
        "\n",
        "            x = torch.FloatTensor(x)\n",
        "            mel = torch.FloatTensor(mel.T).unsqueeze(0)\n",
        "\n",
        "            return x, mel, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-fbDQBkTtDV"
      },
      "source": [
        "## Wav2Lip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9_uXxk-TuPJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class Conv2d(nn.Module):\n",
        "    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        ########TODO######################\n",
        "        # 按下面的网络结构要求，补全代码\n",
        "        # self.conv_block: Sequential结构，Conv2d+BatchNorm\n",
        "        # self.act: relu激活函数\n",
        "        self.conv_block = nn.Sequential(\n",
        "                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\n",
        "                            nn.BatchNorm2d(cout)\n",
        "                            )\n",
        "        self.act = nn.ReLU()\n",
        "        self.residual = residual\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv_block(x)\n",
        "        if self.residual:\n",
        "            out += x\n",
        "        return self.act(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obIDIPVOUU3N"
      },
      "outputs": [],
      "source": [
        "%cd Wav2Lip\n",
        "ds=Dataset(\"train\")\n",
        "x,mel,t=ds[0]\n",
        "print(x.shape)\n",
        "print(mel.shape)\n",
        "print(t.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5xe_fYWUaIf"
      },
      "outputs": [],
      "source": [
        "logloss = nn.BCELoss()\n",
        "    d = nn.functional.cosine_similarity(a, v)\n",
        "    loss = logloss(d.unsqueeze(1), y)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gmanA2HUrN4"
      },
      "outputs": [],
      "source": [
        "def train(device, model, train_data_loader, test_data_loader, optimizer,\n",
        "          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n",
        "\n",
        "    print(\"starting training\")\n",
        "    global global_step, global_epoch\n",
        "    resumed_step = global_step\n",
        "\n",
        "    while global_epoch < nepochs:\n",
        "        running_loss = 0.\n",
        "        prog_bar = tqdm(enumerate(train_data_loader))\n",
        "        for step, (x, mel, y) in prog_bar:\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x = x.to(device)\n",
        "\n",
        "            mel = mel.to(device)\n",
        "\n",
        "            a, v = model(mel, x)\n",
        "            y = y.to(device)\n",
        "\n",
        "            loss = cosine_loss(a, v, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "            global_step += 1\n",
        "            cur_session_steps = global_step - resumed_step\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if global_step == 1 or global_step % checkpoint_interval == 0:\n",
        "                save_checkpoint(\n",
        "                    model, optimizer, global_step, checkpoint_dir, global_epoch)\n",
        "\n",
        "            if global_step % hparams.syncnet_eval_interval == 0:\n",
        "                with torch.no_grad():\n",
        "                    eval_model(test_data_loader, global_step, device, model, checkpoint_dir)\n",
        "\n",
        "            prog_bar.set_description('Epoch: {} Loss: {}'.format(global_epoch, running_loss / (step + 1)))\n",
        "\n",
        "        global_epoch += 1\n",
        "\n",
        "def eval_model(test_data_loader, global_step, device, model, checkpoint_dir):\n",
        "    eval_steps = 1400\n",
        "    print('Evaluating for {} steps'.format(eval_steps))\n",
        "    losses = []\n",
        "    while 1:\n",
        "        for step, (x, mel, y) in enumerate(test_data_loader):\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            x = x.to(device)\n",
        "\n",
        "            mel = mel.to(device)\n",
        "\n",
        "            a, v = model(mel, x)\n",
        "            y = y.to(device)\n",
        "\n",
        "            loss = cosine_loss(a, v, y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            if step > eval_steps: break\n",
        "\n",
        "        averaged_loss = sum(losses) / len(losses)\n",
        "        print(averaged_loss)\n",
        "\n",
        "        return\n",
        "\n",
        "#latest_checkpoint_path = 'checkop'\n",
        "def save_checkpoint(model, optimizer, step, checkpoint_dir, epoch):\n",
        "    global latest_checkpoint_path\n",
        "\n",
        "    checkpoint_path = join(\n",
        "        checkpoint_dir, \"checkpoint_step{:09d}.pth\".format(global_step))\n",
        "    optimizer_state = optimizer.state_dict() if hparams.save_optimizer_state else None\n",
        "    torch.save({\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer_state,\n",
        "        \"global_step\": step,\n",
        "        \"global_epoch\": epoch,\n",
        "    }, checkpoint_path)\n",
        "    latest_checkpoint_path = checkpoint_path\n",
        "    print(\"Saved checkpoint:\", checkpoint_path)\n",
        "\n",
        "def _load(checkpoint_path):\n",
        "    if use_cuda:\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "    else:\n",
        "        checkpoint = torch.load(checkpoint_path,\n",
        "                                map_location=lambda storage, loc: storage)\n",
        "    return checkpoint\n",
        "\n",
        "def load_checkpoint(path, model, optimizer, reset_optimizer=False):\n",
        "    global global_step\n",
        "    global global_epoch\n",
        "\n",
        "    print(\"Load checkpoint from: {}\".format(path))\n",
        "    checkpoint = _load(path)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    if not reset_optimizer:\n",
        "        optimizer_state = checkpoint[\"optimizer\"]\n",
        "        if optimizer_state is not None:\n",
        "            print(\"Load optimizer state from {}\".format(path))\n",
        "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    global_step = checkpoint[\"global_step\"]\n",
        "    global_epoch = checkpoint[\"global_epoch\"]\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZLbHL6QU0xk"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = \"expert_checkpoints/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aH2tMfNU6BW"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = \"/expert_checkpoints/\"\n",
        "checkpoint_path = '/expert_checkpoints/checkpoint_step000070000.pth'\n",
        "if not os.path.exists(checkpoint_dir): os.mkdir(checkpoint_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5T6iXsbVDqu"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset('train')\n",
        "test_dataset = Dataset('val')\n",
        "\n",
        "\n",
        "train_data_loader = data_utils.DataLoader(\n",
        "    train_dataset, batch_size=hparams.batch_size, shuffle=True,\n",
        "    num_workers=2)\n",
        "\n",
        "test_data_loader = data_utils.DataLoader(\n",
        "    test_dataset, batch_size=hparams.batch_size,\n",
        "    num_workers=2)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "# Model\n",
        "model = SyncNet().to(device)\n",
        "print('total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
        "\n",
        "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
        "                       lr=1e-5)\n",
        "\n",
        "if checkpoint_path is not None:\n",
        "    load_checkpoint(checkpoint_path, model, optimizer, reset_optimizer=True)\n",
        "\n",
        "train(device, model, train_data_loader, test_data_loader, optimizer,\n",
        "      checkpoint_dir=checkpoint_dir,\n",
        "      checkpoint_interval=hparams.syncnet_checkpoint_interval,\n",
        "      nepochs=150) #change it to 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8Klw7a0VWEU"
      },
      "outputs": [],
      "source": [
        "%cd /Wav2Lip\n",
        "ds=Dataset(\"train\")\n",
        "x, indiv_mels, mel, y=ds[0]\n",
        "print(x.shape)\n",
        "print(indiv_mels.shape)\n",
        "print(mel.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJyoTq5ZVZGW"
      },
      "outputs": [],
      "source": [
        "logloss = nn.BCELoss()\n",
        "def cosine_loss(a, v, y):\n",
        "    d = nn.functional.cosine_similarity(a, v)\n",
        "    loss = logloss(d.unsqueeze(1), y)\n",
        "\n",
        "    return loss\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "syncnet = SyncNet().to(device)\n",
        "for p in syncnet.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "\n",
        "#####L1 loss\n",
        "recon_loss = nn.L1Loss()\n",
        "def get_sync_loss(mel, g):\n",
        "    g = g[:, :, :, g.size(3)//2:]\n",
        "    g = torch.cat([g[:, :, i] for i in range(syncnet_T)], dim=1)\n",
        "    # B, 3 * T, H//2, W\n",
        "    a, v = syncnet(mel, g)\n",
        "    y = torch.ones(g.size(0), 1).float().to(device)\n",
        "    return cosine_loss(a, v, y)\n",
        "\n",
        "def train(device, model, disc, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n",
        "          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n",
        "    print(\"start training\")\n",
        "    global global_step, global_epoch\n",
        "    resumed_step = global_step\n",
        "    #hparams.syncnet_wt+=5\n",
        "\n",
        "    while nepochs>0:\n",
        "        nepochs-=1\n",
        "        print('Starting Epoch: {}'.format(global_epoch))\n",
        "        running_sync_loss, running_l1_loss, disc_loss, running_perceptual_loss = 0., 0., 0., 0.\n",
        "        running_disc_real_loss, running_disc_fake_loss = 0., 0.\n",
        "        prog_bar = tqdm(enumerate(train_data_loader))\n",
        "        for step, (x, indiv_mels, mel, gt) in prog_bar:\n",
        "            disc.train()\n",
        "            model.train()\n",
        "\n",
        "            x = x.to(device)\n",
        "            mel = mel.to(device)\n",
        "            indiv_mels = indiv_mels.to(device)\n",
        "            gt = gt.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            disc_optimizer.zero_grad()\n",
        "\n",
        "            g = model(indiv_mels, x)\n",
        "\n",
        "            if hparams.syncnet_wt > 0.:\n",
        "                sync_loss = get_sync_loss(mel, g)\n",
        "            else:\n",
        "                sync_loss = 0.\n",
        "\n",
        "            if hparams.disc_wt > 0.:\n",
        "                perceptual_loss = disc.perceptual_forward(g)\n",
        "            else:\n",
        "                perceptual_loss = 0.\n",
        "\n",
        "            l1loss = recon_loss(g, gt)#l1 loss\n",
        "\n",
        "            loss = hparams.syncnet_wt * sync_loss + hparams.disc_wt * perceptual_loss + \\\n",
        "                                    (1. - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            disc_optimizer.zero_grad()\n",
        "\n",
        "            pred = disc(gt)\n",
        "            disc_real_loss = F.binary_cross_entropy(pred, torch.ones((len(pred), 1)).to(device))\n",
        "            disc_real_loss.backward()\n",
        "\n",
        "            pred = disc(g.detach())\n",
        "            disc_fake_loss = F.binary_cross_entropy(pred, torch.zeros((len(pred), 1)).to(device))\n",
        "            disc_fake_loss.backward()\n",
        "\n",
        "            disc_optimizer.step()\n",
        "\n",
        "            running_disc_real_loss += disc_real_loss.item()\n",
        "            running_disc_fake_loss += disc_fake_loss.item()\n",
        "\n",
        "            # Logs\n",
        "            global_step += 1\n",
        "            cur_session_steps = global_step - resumed_step\n",
        "\n",
        "            running_l1_loss += l1loss.item()\n",
        "            if hparams.syncnet_wt > 0.:\n",
        "                running_sync_loss += sync_loss.item()\n",
        "            else:\n",
        "                running_sync_loss += 0.\n",
        "\n",
        "            if hparams.disc_wt > 0.:\n",
        "                running_perceptual_loss += perceptual_loss.item()\n",
        "            else:\n",
        "                running_perceptual_loss += 0.\n",
        "\n",
        "            if global_step == 1 or global_step % checkpoint_interval == 0:\n",
        "                save_checkpoint(\n",
        "                    model, optimizer, global_step, checkpoint_dir, global_epoch)\n",
        "                save_checkpoint(disc, disc_optimizer, global_step, checkpoint_dir, global_epoch, prefix='disc_')\n",
        "\n",
        "\n",
        "            if global_step % hparams.eval_interval == 0:\n",
        "                with torch.no_grad():\n",
        "                    average_sync_loss = eval_model(test_data_loader, global_step, device, model, disc)\n",
        "\n",
        "                    if average_sync_loss < .75:\n",
        "                        hparams.set_hparam('syncnet_wt', 0.03)\n",
        "\n",
        "            prog_bar.set_description('L1: {}, Sync: {}, Percep: {} | Fake: {}, Real: {}'.format(running_l1_loss / (step + 1),\n",
        "                                                                                        running_sync_loss / (step + 1),\n",
        "                                                                                        running_perceptual_loss / (step + 1),\n",
        "                                                                                        running_disc_fake_loss / (step + 1),\n",
        "                                                                                        running_disc_real_loss / (step + 1)))\n",
        "\n",
        "        global_epoch += 1\n",
        "\n",
        "def eval_model(test_data_loader, global_step, device, model, disc):\n",
        "    eval_steps = 300\n",
        "    print('Evaluating for {} steps'.format(eval_steps))\n",
        "    running_sync_loss, running_l1_loss, running_disc_real_loss, running_disc_fake_loss, running_perceptual_loss = [], [], [], [], []\n",
        "    while 1:\n",
        "        for step, (x, indiv_mels, mel, gt) in enumerate((test_data_loader)):\n",
        "            model.eval()\n",
        "            disc.eval()\n",
        "\n",
        "            x = x.to(device)\n",
        "            mel = mel.to(device)\n",
        "            indiv_mels = indiv_mels.to(device)\n",
        "            gt = gt.to(device)\n",
        "\n",
        "            pred = disc(gt)\n",
        "            disc_real_loss = F.binary_cross_entropy(pred, torch.ones((len(pred), 1)).to(device))\n",
        "\n",
        "            g = model(indiv_mels, x)\n",
        "            pred = disc(g)\n",
        "            disc_fake_loss = F.binary_cross_entropy(pred, torch.zeros((len(pred), 1)).to(device))\n",
        "\n",
        "            running_disc_real_loss.append(disc_real_loss.item())\n",
        "            running_disc_fake_loss.append(disc_fake_loss.item())\n",
        "\n",
        "            sync_loss = get_sync_loss(mel, g)\n",
        "\n",
        "            if hparams.disc_wt > 0.:\n",
        "                perceptual_loss = disc.perceptual_forward(g)\n",
        "            else:\n",
        "                perceptual_loss = 0.\n",
        "\n",
        "            l1loss = recon_loss(g, gt)\n",
        "\n",
        "            loss = hparams.syncnet_wt * sync_loss + hparams.disc_wt * perceptual_loss + \\\n",
        "                                    (1. - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n",
        "\n",
        "            running_l1_loss.append(l1loss.item())\n",
        "            running_sync_loss.append(sync_loss.item())\n",
        "\n",
        "            if hparams.disc_wt > 0.:\n",
        "                running_perceptual_loss.append(perceptual_loss.item())\n",
        "            else:\n",
        "                running_perceptual_loss.append(0.)\n",
        "\n",
        "            if step > eval_steps: break\n",
        "\n",
        "        print('L1: {}, Sync: {}, Percep: {} | Fake: {}, Real: {}'.format(sum(running_l1_loss) / len(running_l1_loss),\n",
        "                                                            sum(running_sync_loss) / len(running_sync_loss),\n",
        "                                                            sum(running_perceptual_loss) / len(running_perceptual_loss),\n",
        "                                                            sum(running_disc_fake_loss) / len(running_disc_fake_loss),\n",
        "                                                             sum(running_disc_real_loss) / len(running_disc_real_loss)))\n",
        "        return sum(running_sync_loss) / len(running_sync_loss)\n",
        "\n",
        "latest_wav2lip_checkpoint = '/kaggle/input/wav2lip-hq-lombardgrid/wav2lip_checkpoints/checkpoint_step000279000.pth'\n",
        "def save_checkpoint(model, optimizer, step, checkpoint_dir, epoch, prefix=''):\n",
        "    global latest_wav2lip_checkpoint\n",
        "    checkpoint_path = join(\n",
        "        checkpoint_dir, \"{}checkpoint_step{:09d}.pth\".format(prefix, global_step))\n",
        "    if 'disc' not in checkpoint_path:\n",
        "        latest_wav2lip_checkpoint = checkpoint_path\n",
        "    optimizer_state = optimizer.state_dict() if hparams.save_optimizer_state else None\n",
        "    torch.save({\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer_state,\n",
        "        \"global_step\": step,\n",
        "        \"global_epoch\": epoch,\n",
        "    }, checkpoint_path)\n",
        "    print(\"Saved checkpoint:\", checkpoint_path)\n",
        "\n",
        "def _load(checkpoint_path):\n",
        "    if use_cuda:\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "    else:\n",
        "        checkpoint = torch.load(checkpoint_path,\n",
        "                                map_location=lambda storage, loc: storage)\n",
        "    return checkpoint\n",
        "\n",
        "\n",
        "def load_checkpoint(path, model, optimizer, reset_optimizer=False, overwrite_global_states=True):\n",
        "    global global_step\n",
        "    global global_epoch\n",
        "\n",
        "    print(\"Load checkpoint from: {}\".format(path))\n",
        "    checkpoint = _load(path)\n",
        "    s = checkpoint[\"state_dict\"]\n",
        "    new_s = {}\n",
        "    for k, v in s.items():\n",
        "        new_s[k.replace('module.', '')] = v\n",
        "    model.load_state_dict(new_s) #modified line\n",
        "    if not reset_optimizer:\n",
        "        optimizer_state = checkpoint[\"optimizer\"]\n",
        "        if optimizer_state is not None:\n",
        "            print(\"Load optimizer state from {}\".format(path))\n",
        "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    if overwrite_global_states:\n",
        "        global_step = checkpoint[\"global_step\"]\n",
        "        global_epoch = checkpoint[\"global_epoch\"]\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKxn_BGIVn_c"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = \"/wav2lip_checkpoints\"  #checkpoint 存储的位置\n",
        "\n",
        "# Dataset and Dataloader setup\n",
        "train_dataset = Dataset('train')\n",
        "test_dataset = Dataset('val')\n",
        "\n",
        "train_data_loader = data_utils.DataLoader(\n",
        "    train_dataset, batch_size=hparams.batch_size, shuffle=True,\n",
        "    num_workers=2)\n",
        "\n",
        "test_data_loader = data_utils.DataLoader(\n",
        "    test_dataset, batch_size=hparams.batch_size,\n",
        "    num_workers=2)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        " # Model\n",
        "model = Wav2Lip().to(device)\n",
        "disc = Wav2Lip_disc_qual().to(device)\n",
        "\n",
        "print('total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
        "print('total DISC trainable params {}'.format(sum(p.numel() for p in disc.parameters() if p.requires_grad)))\n",
        "\n",
        "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
        "                       lr=hparams.initial_learning_rate,\n",
        "                       betas=(0.5, 0.999))\n",
        "disc_optimizer = optim.Adam([p for p in disc.parameters() if p.requires_grad],\n",
        "                            lr=hparams.disc_initial_learning_rate,\n",
        "                            betas=(0.5, 0.999))\n",
        "\n",
        "\n",
        "checkpoint_path = \"/wav2lip-hq-lombardgrid/wav2lip_checkpoints/checkpoint_step000285000.pth\"\n",
        "model = load_checkpoint(checkpoint_path, model, optimizer, reset_optimizer=False)\n",
        "\n",
        "syncnet_checkpoint_path = \"/wav2lip24epoch/expert_checkpoints/checkpoint_step000060000.pth\"\n",
        "\n",
        "try:\n",
        "    if \".pth\" in latest_checkpoint_path:\n",
        "\n",
        "        syncnet_checkpoint_path = \"/kaggle/input/wav2lip-expertsync-lombarddata/expert_checkpoints/checkpoint_step000100000.pth\"#latest_checkpoint_path\n",
        "except:\n",
        "    syncnet_checkpoint_path = latest_checkpoint_path = \"/kaggle/input/wav2lip-expertsync-lombarddata/expert_checkpoints/checkpoint_step000100000.pth\"\n",
        "    pass\n",
        "load_checkpoint(syncnet_checkpoint_path, syncnet, None, reset_optimizer=True,\n",
        "                            overwrite_global_states=False)\n",
        "\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.mkdir(checkpoint_dir)\n",
        "\n",
        "# Train!\n",
        "\n",
        "train(device, model, disc, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n",
        "          checkpoint_dir=checkpoint_dir,\n",
        "          checkpoint_interval=hparams.checkpoint_interval,\n",
        "          nepochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "407MyP0cV9In"
      },
      "outputs": [],
      "source": [
        "checkpoint_path=\"/wav2lip_checkpoints/checkpoint_step000000001.pth\"\n",
        "checkpoint_path = latest_wav2lip_checkpoint\n",
        "face=\"/wave2lip/wav2lip_homework/input_video.mp4\"\n",
        "speech=\"/feamlevoice/content/wavs/0seg_10000.wav\"\n",
        "resize_factor=1\n",
        "crop=[0,-1,0,-1]\n",
        "fps=25\n",
        "static=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dSbJBltWGMk"
      },
      "outputs": [],
      "source": [
        "if not os.path.isfile(face):\n",
        "    raise ValueError('--face argument must be a valid path to video/image file')\n",
        "\n",
        "\n",
        "else:\n",
        "    video_stream = cv2.VideoCapture(face)\n",
        "    fps = video_stream.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    print('Reading video frames...')\n",
        "\n",
        "    full_frames = []\n",
        "    while 1:\n",
        "        still_reading, frame = video_stream.read()\n",
        "        if not still_reading:\n",
        "            video_stream.release()\n",
        "            break\n",
        "        if resize_factor > 1:\n",
        "            frame = cv2.resize(frame, (frame.shape[1]//resize_factor, frame.shape[0]//resize_factor))\n",
        "\n",
        "\n",
        "\n",
        "        y1, y2, x1, x2 =crop\n",
        "        if x2 == -1: x2 = frame.shape[1]\n",
        "        if y2 == -1: y2 = frame.shape[0]\n",
        "\n",
        "        frame = frame[y1:y2, x1:x2]\n",
        "\n",
        "        full_frames.append(frame)\n",
        "\n",
        "print (\"Number of frames available for inference: \"+str(len(full_frames)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOIlkrhYWPE7"
      },
      "outputs": [],
      "source": [
        "if not speech.endswith('.wav'):\n",
        "    print('Extracting raw audio...')\n",
        "    command = 'ffmpeg -y -i {} -strict -2 {}'.format(speech, 'temp/temp.wav')\n",
        "\n",
        "    subprocess.call(command, shell=True)\n",
        "    speech = 'temp/temp.wav'\n",
        "\n",
        "wav = audio.load_wav(speech, 16000)\n",
        "mel = audio.melspectrogram(wav)\n",
        "print(mel.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYvJDR2eWQlu"
      },
      "outputs": [],
      "source": [
        "wav2lip_batch_size=128\n",
        "mel_step_size=16\n",
        "\n",
        "mel_chunks = []\n",
        "mel_idx_multiplier = 80./fps\n",
        "i = 0\n",
        "while 1:\n",
        "    start_idx = int(i * mel_idx_multiplier)\n",
        "    if start_idx + mel_step_size > len(mel[0]):\n",
        "        mel_chunks.append(mel[:, len(mel[0]) - mel_step_size:])\n",
        "        break\n",
        "    mel_chunks.append(mel[:, start_idx : start_idx + mel_step_size])\n",
        "    i += 1\n",
        "\n",
        "print(\"Length of mel chunks: {}\".format(len(mel_chunks)))\n",
        "\n",
        "full_frames = full_frames[:len(mel_chunks)]\n",
        "\n",
        "batch_size = wav2lip_batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBZ4ONm_WTNA"
      },
      "outputs": [],
      "source": [
        "img_size = 96\n",
        "pads=[0,20,0,0]\n",
        "nosmooth=False\n",
        "face_det_batch_size=16\n",
        "\n",
        "def get_smoothened_boxes(boxes, T):\n",
        "    for i in range(len(boxes)):\n",
        "        if i + T > len(boxes):\n",
        "            window = boxes[len(boxes) - T:]\n",
        "        else:\n",
        "            window = boxes[i : i + T]\n",
        "        boxes[i] = np.mean(window, axis=0)\n",
        "    return boxes\n",
        "\n",
        "def face_detect(images):\n",
        "    detector = face_detection.FaceAlignment(face_detection.LandmarksType._2D,\n",
        "                                            flip_input=False, device=device)\n",
        "\n",
        "    batch_size = face_det_batch_size\n",
        "\n",
        "    while 1:\n",
        "        predictions = []\n",
        "        try:\n",
        "            for i in tqdm(range(0, len(images), batch_size)):\n",
        "                predictions.extend(detector.get_detections_for_batch(np.array(images[i:i + batch_size])))\n",
        "        except RuntimeError:\n",
        "            if batch_size == 1:\n",
        "                raise RuntimeError('Image too big to run face detection on GPU. Please use the --resize_factor argument')\n",
        "            batch_size //= 2\n",
        "            print('Recovering from OOM error; New batch size: {}'.format(batch_size))\n",
        "            continue\n",
        "        break\n",
        "\n",
        "    results = []\n",
        "    pady1, pady2, padx1, padx2 = pads\n",
        "    for rect, image in zip(predictions, images):\n",
        "        if rect is None:\n",
        "            cv2.imwrite('temp/faulty_frame.jpg', image) # check this frame where the face was not detected.\n",
        "            raise ValueError('Face not detected! Ensure the video contains a face in all the frames.')\n",
        "\n",
        "        y1 = max(0, rect[1] - pady1)\n",
        "        y2 = min(image.shape[0], rect[3] + pady2)\n",
        "        x1 = max(0, rect[0] - padx1)\n",
        "        x2 = min(image.shape[1], rect[2] + padx2)\n",
        "\n",
        "        results.append([x1, y1, x2, y2])\n",
        "\n",
        "    boxes = np.array(results)\n",
        "    if not nosmooth: boxes = get_smoothened_boxes(boxes, T=5)\n",
        "    results = [[image[y1: y2, x1:x2], (y1, y2, x1, x2)] for image, (x1, y1, x2, y2) in zip(images, boxes)]\n",
        "\n",
        "    del detector\n",
        "    return results\n",
        "\n",
        "box=[-1,-1,-1,-1]\n",
        "\n",
        "def datagen(frames, mels):\n",
        "    img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n",
        "\n",
        "    if box[0] == -1:\n",
        "        if not static:\n",
        "            face_det_results = face_detect(frames)\n",
        "        else:\n",
        "            face_det_results = face_detect([frames[0]])\n",
        "    else:\n",
        "        print('Using the specified bounding box instead of face detection...')\n",
        "        y1, y2, x1, x2 = box\n",
        "        face_det_results = [[f[y1: y2, x1:x2], (y1, y2, x1, x2)] for f in frames]\n",
        "\n",
        "    for i, m in enumerate(mels):\n",
        "        idx = 0 if static else i%len(frames)\n",
        "        frame_to_save = frames[idx].copy()\n",
        "        face, coords = face_det_results[idx].copy()\n",
        "\n",
        "        face = cv2.resize(face, (img_size, img_size))\n",
        "\n",
        "        img_batch.append(face)\n",
        "        mel_batch.append(m)\n",
        "        frame_batch.append(frame_to_save)\n",
        "        coords_batch.append(coords)\n",
        "\n",
        "        if len(img_batch) >= wav2lip_batch_size:\n",
        "            img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n",
        "\n",
        "            img_masked = img_batch.copy()\n",
        "            img_masked[:, img_size//2:] = 0\n",
        "\n",
        "            img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n",
        "            mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
        "\n",
        "            yield img_batch, mel_batch, frame_batch, coords_batch\n",
        "            img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n",
        "\n",
        "    if len(img_batch) > 0:\n",
        "        img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n",
        "\n",
        "        img_masked = img_batch.copy()\n",
        "        img_masked[:, img_size//2:] = 0\n",
        "\n",
        "        img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n",
        "        mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
        "\n",
        "        yield img_batch, mel_batch, frame_batch, coords_batch\n",
        "\n",
        "mel_step_size = 16\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} for inference.'.format(device))\n",
        "\n",
        "\n",
        "def _load(checkpoint_path):\n",
        "    if device == 'cuda':\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "    else:\n",
        "        checkpoint = torch.load(checkpoint_path,\n",
        "                                map_location=lambda storage, loc: storage)\n",
        "    return checkpoint\n",
        "\n",
        "def load_model(path):\n",
        "    model = Wav2Lip()\n",
        "    print(\"Load checkpoint from: {}\".format(path))\n",
        "    checkpoint = _load(path)\n",
        "    s = checkpoint[\"state_dict\"]\n",
        "    new_s = {}\n",
        "    for k, v in s.items():\n",
        "        new_s[k.replace('module.', '')] = v\n",
        "    model.load_state_dict(new_s)\n",
        "\n",
        "    model = model.to(device)\n",
        "    return model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_OX6sQrWhOY"
      },
      "outputs": [],
      "source": [
        "full_frames = full_frames[:len(mel_chunks)]\n",
        "\n",
        "batch_size = wav2lip_batch_size\n",
        "gen = datagen(full_frames.copy(), mel_chunks)\n",
        "\n",
        "for i, (img_batch, mel_batch, frames, coords) in enumerate(tqdm(gen,\n",
        "                                        total=int(np.ceil(float(len(mel_chunks))/batch_size)))):\n",
        "    if i == 0:\n",
        "        model = load_model(checkpoint_path)\n",
        "        print (\"Model loaded\")\n",
        "\n",
        "        frame_h, frame_w = full_frames[0].shape[:-1]\n",
        "        out = cv2.VideoWriter('/kaggle/working/temp/result_without_audio.mp4',\n",
        "                                cv2.VideoWriter_fourcc(*'DIVX'), fps, (frame_w, frame_h))\n",
        "\n",
        "    img_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(device)\n",
        "    mel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(device)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = model(mel_batch, img_batch)\n",
        "\n",
        "    pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.\n",
        "\n",
        "    for p, f, c in zip(pred, frames, coords):\n",
        "        y1, y2, x1, x2 = c\n",
        "        p = cv2.resize(p.astype(np.uint8), (x2 - x1, y2 - y1))\n",
        "\n",
        "        f[y1:y2, x1:x2] = p\n",
        "        out.write(f)\n",
        "\n",
        "out.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1bUszVIWi6Y"
      },
      "outputs": [],
      "source": [
        "outfile=\"/result/result.mp4\"\n",
        "command = 'ffmpeg -y -i {} -i {} -strict -2 -q:v 1 {}'.format(speech, 'temp/result_without_audio.mp4',outfile)\n",
        "subprocess.call(command, shell=platform.system() != 'Windows')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
